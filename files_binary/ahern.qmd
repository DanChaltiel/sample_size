---
title: "A'Hern design for a 1-sample Test"
author: "Dan Chaltiel"
format: gfm
editor: visual
---

This section addresses methods to compute the sample size for a single-stage one-sample A'Hern design.

## Custom function

![](https://img.shields.io/badge/Validation-95%25-green.svg)

The following function is an implementation of the [A'Hern design](https://stat.ethz.ch/education/semesters/as2012/bio/ahernSampleSize.pdf).

```{r}
#' Compute exact single-stage sample size using A'Hern's design
#'
#' This function computes the optimal sample size `n` and decision threshold `r`
#' for a single-stage phase II design based on A'Hern (2001), using the exact
#' binomial distribution. 
#'
#' @param p0 Numeric. The unacceptable response rate under the null hypothesis (H0).
#' @param p1 Numeric. The desirable minimal response rate under the alternative hypothesis (H1).
#' @param alpha Numeric. One-sided type I error rate. 
#' @param power Numeric. Desired statistical power (1 - type II error). 
#' @param n_min,n_max Integer. Minimum & maximum sample size to consider. 
#' @param tolerance Numeric. Acceptable numerical slack for comparison of alpha and power. Default is 0.001.
#'
#' @references
#' A’Hern RP. Sample size tables for exact single-stage phase II designs. Stat Med. 2001;20(6):859–866.
#' https://stat.ethz.ch/education/semesters/as2012/bio/ahernSampleSize.pdf
sample_size_ahern = function(p0, p1, alpha=0.05, power=0.8, 
                             n_min=1, n_max=200, 
                             tolerance=0.00005) {
  stopifnot(p0<p1)
  rtn = tidyr::expand_grid(n = n_min:n_max,
                    r = 0:n_max) |>
    dplyr::filter(r <= n) |>
    dplyr::mutate(
      alpha_real = 1 - pbinom(r - 1, size = n, prob = p0),
      power_real = 1 - pbinom(r - 1, size = n, prob = p1),
    ) |>
    dplyr::filter(alpha_real <= alpha+tolerance, 
                  power_real >= power-tolerance) |>
    dplyr::slice_min(r, by=n) |>
    dplyr::slice_min(n) |>
    as.data.frame()
  if(nrow(rtn)==0) {
    warning("Increase `n_max` ", 
            "p0=", p0, ", p1=",p1, ", alpha=",alpha, ", power=",power)
  }
  rtn
}
```

### One-line example

The function outputs a dataframe with input probabilities and alpha, the actual attained power, and the total sample size. Each arm will therefore hold `n_total/2` patients.

```{r}
sample_size_ahern(p0=0.35, p1=0.55, alpha=0.05, power=0.8)
sample_size_ahern(p0=0.15, p1=0.3, alpha=0.1, power=0.9)
```

### Grid search

This example uses `expand_grid()` to calculate the sample size for an abitrary number of scenarios.

```{r}
#| message: false
#| warning: false
library(tidyverse)
rslt = expand_grid(p0=c(0.5,0.6),
                   p1=c(0.7,0.8),
                   power=c(0.8, 0.9)
) %>%
  filter(p0<p1) %>% 
  rowwise() %>% 
  mutate(tbl = list(sample_size_ahern(p0=p0, p1=p1, power=power))) %>% 
  unnest(tbl)
rslt
```

### Tolerance

As you can see, the real type I error rate can be slightly higher than 5%, as in the princeps article. Use the `tolerance` argument if you need to be strict:

```{r}
sample_size_ahern(p0=0.6, p1=0.7, alpha=0.05, power=0.9)
sample_size_ahern(p0=0.6, p1=0.7, alpha=0.05, power=0.9, tolerance=0)
```

## Validation

By scraping the article, we can validate that the calculation is correct.

The article used MS Excel BINOMDIST function to compute the probabilities, which admittedly failed for large N and was then replaced by a normal approximation. As it never fails in R, this approximation was not implemented. Most differences are very likely to come from this fact.

The other difference is the `tolerance` argument. In the original article, `alpha_real` and `power_real` seem to be rounded, but this is not detailled. Differences in smaller N can be

```{r}
ahern_table = read_lines("ahern_table.txt") %>% 
  as_tibble_col() %>% 
  separate_wider_delim (value, 
                        names=c("p0", "p1", "a0.05_p0.8", "a0.05_p0.9", 
                                "a0.01_p0.8", "a0.01_p0.9"),
                        too_few = "align_end",
                        delim=" ") %>% 
  filter(!is.na(p1)) %>% 
  fill(p0, .direction="down") %>% 
  pivot_longer(-c(p0, p1), names_to=c("alpha", "power"), names_pattern="a(.*)_p(.*)") %>% 
  separate(value, into=c("r", "n"), sep="=") %>% 
  mutate_all(as.numeric)

comparison = ahern_table %>% 
  rowwise() %>%
  mutate(
    ah = list(sample_size_ahern(p0=p0, p1=p1, alpha=alpha, power=power, n_min=n-15, n_max=n+15,
                                tolerance=0.00005) %>% suppressWarnings())
  ) %>% 
  unnest(ah, names_sep="_", keep_empty=TRUE) %>% 
  mutate(diff_n = ah_n-n, diff_r = ah_r-r)
```

Sample sizes are the same in \>95% cases:

```{r}
comparison %>% 
  summarise(
    prop_correct__n = mean(!is.na(diff_n) & diff_n==0),
    prop_correct__r = mean(!is.na(diff_r) & diff_r==0),
    mean_diff__n = mean(diff_n[diff_n!=0], na.rm=TRUE),
    mean_diff__r = mean(diff_r[diff_r!=0], na.rm=TRUE)
  ) %>% 
  pivot_longer(everything(), names_sep="__", names_to=c("variable", ".value"))
```

Here are the cases with a difference:

```{r}
comparison %>% 
  arrange(desc(pmax(diff_n, diff_r))) %>% 
  filter(diff_n!=0 | diff_r!=0 | is.na(diff_n) | is.na(diff_r)) %>% 
  print(n=Inf)

#differences in small N are due to rounding
sample_size_ahern(p0=0.05, p1=0.15, alpha=0.05, power=0.9,
                  tolerance=0.0005)
```
